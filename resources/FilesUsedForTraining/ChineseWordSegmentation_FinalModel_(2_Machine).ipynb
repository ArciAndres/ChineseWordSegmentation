{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4HVu_d9STkL8"
   },
   "source": [
    "# ChineseWordSegmentation - FinalModel (2 Machine)\n",
    "Final training for the delivery of the homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rYKChoi7itqE"
   },
   "source": [
    "After having perfomed the training with dataset MSR and PKU, we realized that the test could be done with any of the data in the datasets availabe, so we will train a model with a parte of the complete datasets and try to come up with a general network representation to predict the labels accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "sQ26beqeiLpq",
    "outputId": "3d41dfc8-e1ac-4c2b-e4ce-0b7cc82a7919"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "#drive.mount(\"/content/gdrive\", force_remount=True)\n",
    "\n",
    "import sys\n",
    "root_path = '/content/gdrive/My Drive/Sapienza/NLP/HM1/Arci/'  #change dir to your project folder\n",
    "sys.path.insert(0, root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qw6hLV6_jErz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, TimeDistributed, concatenate, Activation, Masking, Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "import os, datetime, time\n",
    "\n",
    "import ChinesePreprocess as CP\n",
    "import ModelConfiguration as ModelConfig\n",
    "import TrainingUtils\n",
    "from code_provided.score import score\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bc4qYBRJjbNL"
   },
   "source": [
    "# Preprocess Initialization\n",
    "For all the general cases here, the reordered datasets will be used, and the vocabulary according to each dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qYtFYBFvjw87"
   },
   "outputs": [],
   "source": [
    "# Model to TPU (Only run when TPU is activated)\n",
    "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "def toTPU(mdl):\n",
    "  tpu_model = tf.contrib.tpu.keras_to_tpu_model(mdl,strategy=tf.contrib.tpu.TPUDistributionStrategy(tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
    "  return tpu_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cVuK-JEX66u2"
   },
   "source": [
    "The default dataset will be MSR, unless especified different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FQqpJpkij2jj"
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "models = {}\n",
    "modelstpu = {}\n",
    "hists = {}\n",
    "\n",
    "def load_defaults_saved_all():\n",
    "  with open(root_path + \"../data_load/xinput_all_100.pkl\",'rb') as file:\n",
    "    xinput = pickle.load(file)\n",
    "  with open(root_path + \"../data_load/yout_all_100.pkl\",'rb') as file:\n",
    "    yout = pickle.load(file)\n",
    "  with open(root_path + \"../vocabs/vocab_all.pkl\",'rb') as file:\n",
    "    vocab = pickle.load(file)\n",
    "  with open(root_path + \"../data_load/xtest_all_100.pkl\",'rb') as voc:\n",
    "    xtest = pickle.load(voc)\n",
    "  with open(root_path + \"../data_load/ytest_all_100.pkl\",'rb') as file:\n",
    "    ytest = pickle.load(file)\n",
    "    #print(\"Vocabulary loaded.\")\n",
    "  \n",
    "  return xinput, yout, xtest, ytest, vocab\n",
    "\n",
    "#-------------------------------------------\n",
    "########### Callbacks when fitting #########\n",
    "#-------------------------------------------\n",
    "\n",
    "def get_callbacks(model_name):\n",
    "  tf_callback = K.callbacks.TensorBoard(log_dir=root_path+'logs/'+model_name)\n",
    "  time_callback = TrainingUtils.TimeHistory()\n",
    "  csv_logger = K.callbacks.CSVLogger(root_path+'../models/'+model_name+'.log')\n",
    "  checkpoint = K.callbacks.ModelCheckpoint(root_path+'../models/'+model_name+\"_weights-chkp-{epoch:02d}-{val_acc:.2f}.hdf5\", monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "  #earlyStop_callback= K.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "  return tf_callback, time_callback, csv_logger, checkpoint#, earlyStop_callback\n",
    "\n",
    "class train_config():\n",
    "  def __init__(self,tag,dataset,lenvocab,opt='adam', lr=0.001, drop_lstm=0.0, drop_rec=0.0, emb_size1=64, emb_size2=64, batch_size=256, lstm_units=256,maxlength=50):\n",
    "    self.tag=tag\n",
    "    self.dataset=dataset\n",
    "    self.lenvocab=lenvocab\n",
    "    self.opt=opt\n",
    "    self.lr=lr\n",
    "    self.drop_lstm=drop_lstm\n",
    "    self.drop_rec=drop_rec\n",
    "    self.emb_size1=emb_size1\n",
    "    self.emb_size2=emb_size2\n",
    "    self.batch_size=batch_size\n",
    "    self.lstm_units=lstm_units\n",
    "    self.maxlength=maxlength\n",
    "\n",
    "  \n",
    "def get_model_custom(TC): ## TC = Training configuration\n",
    "    \n",
    "    input1 =  Input(shape=(TC.maxlength,))\n",
    "    uni_layer = Embedding(TC.lenvocab,TC.emb_size1, input_length=TC.maxlength, mask_zero=True)(input1)\n",
    "    input2   = Input(shape=(TC.maxlength,))\n",
    "    bi_layer = Embedding(TC.lenvocab,TC.emb_size2, input_length=TC.maxlength, mask_zero=True)(input2)\n",
    "    ngram_layer = concatenate([uni_layer, bi_layer])\n",
    "    lstm_layer = Bidirectional(LSTM(TC.lstm_units, return_sequences=True, dropout=TC.drop_lstm, recurrent_dropout=TC.drop_rec))(ngram_layer)\n",
    "    time_dist_layer = TimeDistributed(Dense(4, activation='softmax'))(lstm_layer)\n",
    "    model = K.models.Model([input1,input2],time_dist_layer)\n",
    "    \n",
    "    if tconfig.opt=='adam':\n",
    "      optim = K.optimizers.Adam(lr=tconfig.lr)\n",
    "    else:\n",
    "      optim = K.optimizers.Adam(lr=tconfig.lr)\n",
    "    \n",
    "    model.compile(optimizer=optim, loss='categorical_crossentropy', metrics = ['acc', tf.keras.metrics.Recall(), tf.keras.metrics.Precision()])\n",
    "    return model\n",
    "  \n",
    "def get_model_name(tc):\n",
    "  model_name = datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%d_')+\"%s_%s_%s_lr%s_drLS%s_drRec%s_emb1%s_emb2%s_batch%s_units%s_maxl%s\" % (str(tc.tag),str(tc.dataset), tc.opt, str(tc.lr), str(tc.drop_lstm), str(tc.drop_rec), str(tc.emb_size1), str(tc.emb_size2), str(tc.batch_size), str(tc.lstm_units), str(tc.maxlength) )\n",
    "  return model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZZ3uH5Qbwu27"
   },
   "source": [
    "## Preprocessing all. Save useful variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9I68vx2WocN3"
   },
   "outputs": [],
   "source": [
    "def load_defaults_all(trainds=\"ALL_training_simp_reordered_shuf\", testds=\"ALL_test_gold_simp_reordered_shuf\", maxlength = 50, vocab_path = \"vocab_all\", samples_porcent =  1, verbose=False):\n",
    "  print(\"Loading configurations.\")\n",
    "  print(\"Training dataset: \", trainds)\n",
    "  print(\"Testing dataset: \", testds)\n",
    "  print(\"Vocabulary: \", vocab_path)\n",
    "  print(\"maxlength: \", maxlength)\n",
    "  \n",
    "  print(\"\\n******************** Loading vocabulary **********************\\n\")\n",
    "  with open(root_path + \"../vocabs/%s.pkl\" % vocab_path,'rb') as voc:\n",
    "    vocab = pickle.load(voc)\n",
    "    print(\"Vocabulary loaded.\")\n",
    "  \n",
    "  print(\"\\n******************** Extract train data (ALL) **********************\\n\")\n",
    "  ds_train = CP.ChinesePreprocess(root_path + \"../dataset/icwb2-data/training/%s.utf8\" % trainds, \n",
    "                                  num_samples=0, # Should be zero mostly. Becasue it's convenient to read everything most of the times. If not zero, some words to build the vocabulary might get lost in the trimming\n",
    "                                  vocabulary = vocab, # Static preset vocabulary. Same in training\n",
    "                                  verbose=False)\n",
    "\n",
    "  print(\"\\n\\n******************** Extract test data (ALL) *******************\\n\")\n",
    "  ds_test = CP.ChinesePreprocess(root_path + \"../dataset/icwb2-data/gold/%s.utf8\" % testds, \n",
    "                                 num_samples=0, \n",
    "                                 vocabulary = vocab,# Same vocabulary of training\n",
    "                                 verbose=False)\n",
    "\n",
    "    # Maximum length for padding\n",
    "  train = CP.ChinesePreprocess.apply_padding_data_and_labels(ds_train, maxlength, False)\n",
    "  test = CP.ChinesePreprocess.apply_padding_data_and_labels(ds_test, maxlength, False)\n",
    "  \n",
    "   # 100% of the samples\n",
    "  print(\"Percentage taken: \", samples_porcent*100,'%')\n",
    "  \n",
    "  # This is for trainings only. Not for testing \n",
    "  k = round(len(train.unigrams_pad)*samples_porcent)\n",
    "  print(\"Number of samples: \", k)\n",
    "  xinput = [train.unigrams_pad[:k], train.bigrams_pad[:k]]\n",
    "  yout = train.labels[:k]\n",
    "  print(\"Done.\")\n",
    "  return train, test, xinput, yout, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "P3yCnNVmoht5",
    "outputId": "46f4a479-5ed3-405b-8808-6c691d718b78"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading configurations.\n",
      "Training dataset:  ALL_training_simp_reordered_shuf\n",
      "Testing dataset:  ALL_test_gold_simp_reordered_shuf\n",
      "Vocabulary:  vocab_all\n",
      "maxlength:  50\n",
      "\n",
      "******************** Loading vocabulary **********************\n",
      "\n",
      "Vocabulary loaded.\n",
      "\n",
      "******************** Extract train data (ALL) **********************\n",
      "\n",
      "[INFO] Using preset vocabulary. No. of elements:  1043604\n",
      "\n",
      "\n",
      "******************** Extract test data (ALL) *******************\n",
      "\n",
      "[INFO] Using preset vocabulary. No. of elements:  1043604\n",
      "Percentage taken:  100 %\n",
      "Number of samples:  336430\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "#train, test, xinput, yout, vocab = load_defaults_all(maxlength=50,samples_porcent=1)\n",
    "#with open(root_path+\"../data_load/xinput_all_100.pkl\", 'wb') as file:\n",
    "#  pickle.dump(xinput, file)\n",
    "#with open(root_path+\"../data_load/yout_all_100.pkl\", 'wb') as file:\n",
    "#  pickle.dump(yout, file)\n",
    "#with open(root_path+\"../data_load/xtest_all_100.pkl\", 'wb') as file:\n",
    "#  pickle.dump([test.unigrams_pad, test.bigrams_pad], file)\n",
    "#with open(root_path+\"../data_load/ytest_all_100.pkl\", 'wb') as file:\n",
    "#  pickle.dump(test.labels, file)\n",
    "#with open(root_path+\"../data_load/vocab_all.pkl\", 'wb') as file:\n",
    "# pickle.dump(vocab, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "51T9Yg1pVmB-",
    "outputId": "501f5342-da0a-4248-bca2-8210b7cbdcfb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "336430"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train.bigrams_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "-PLM7nLElpVj",
    "outputId": "2b951991-ba4b-4449-9306-863755de0cc2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([907116, 124420, 333878, 955007,  79251, 879095, 843694, 907757,\n",
       "       637562, 127326, 869954, 188286, 187659, 333878, 530873, 569864,\n",
       "       869954, 992885, 188286, 217176, 897331, 217176, 955007, 204905,\n",
       "       992885, 188286, 333878,  79251, 702761, 835892,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bi4NSKUeoVnn"
   },
   "source": [
    "# Grid search 2 (All)\n",
    "Based on the experiments with the subsets, a low learning rate, around 0.0005, and a dropout around 0.4 can show good results.\n",
    "We will try that first, and then, if there's time, change the learning rate. We will go for 50 epochs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bv39QAOezk4R"
   },
   "source": [
    "## Training 2.1.1: LR:0.0005, Drop_LSTM=0.4, Drop_Rec=0.4\n",
    "|   Static   | Variable                    |\n",
    "|:----------:|-----------------------------|\n",
    "|  Drop_LSTM = [0.4]   | LR=0.0005, 0.00025 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oy9TjEcUzkGq"
   },
   "outputs": [],
   "source": [
    "xinput, yout, xtest, ytest, vocab = load_defaults_saved_all()\n",
    "#x test is a list of unigram and bigrams like this [unigrams, bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1822
    },
    "colab_type": "code",
    "id": "hDitdIYfzjQN",
    "outputId": "38d2ec08-a203-4952-e6d1-2beaa37a0338"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** Training started on:  2019-04-24 - 17:43:18\n",
      "Configuration: \n",
      "{'tag': '_Final2.1.1(GPU)', 'dataset': 'all100', 'lenvocab': 1043604, 'opt': 'adam', 'lr': 0.0005, 'drop_lstm': 0.4, 'drop_rec': 0.4, 'emb_size1': 64, 'emb_size2': 64, 'batch_size': 512, 'lstm_units': 256, 'maxlength': 50}\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model name: 20190424__Final2.1.1(GPU)_all100_adam_lr0.0005_drLS0.4_drRec0.4_emb164_emb264_batch512_units256_maxl50\n",
      "Train on 336430 samples, validate on 12590 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "336384/336430 [============================>.] - ETA: 0s - loss: 0.3517 - acc: 0.8605 - recall: 0.7935 - precision: 0.9294\n",
      "Epoch 00001: val_acc improved from -inf to 0.92360, saving model to /content/gdrive/My Drive/Sapienza/NLP/HM1/Arci/../models/20190424__Final2.1.1(GPU)_all100_adam_lr0.0005_drLS0.4_drRec0.4_emb164_emb264_batch512_units256_maxl50_weights-chkp-01-0.92.hdf5\n",
      "336430/336430 [==============================] - 242s 721us/sample - loss: 0.3517 - acc: 0.8605 - recall: 0.7935 - precision: 0.9294 - val_loss: 0.2073 - val_acc: 0.9236 - val_recall: 0.9187 - val_precision: 0.9291\n",
      "Epoch 2/50\n",
      "336384/336430 [============================>.] - ETA: 0s - loss: 0.1310 - acc: 0.9494 - recall: 0.9468 - precision: 0.9521\n",
      "Epoch 00002: val_acc improved from 0.92360 to 0.92826, saving model to /content/gdrive/My Drive/Sapienza/NLP/HM1/Arci/../models/20190424__Final2.1.1(GPU)_all100_adam_lr0.0005_drLS0.4_drRec0.4_emb164_emb264_batch512_units256_maxl50_weights-chkp-02-0.93.hdf5\n",
      "336430/336430 [==============================] - 241s 715us/sample - loss: 0.1310 - acc: 0.9494 - recall: 0.9468 - precision: 0.9521 - val_loss: 0.2020 - val_acc: 0.9283 - val_recall: 0.9251 - val_precision: 0.9320\n",
      "Epoch 3/50\n",
      "336384/336430 [============================>.] - ETA: 0s - loss: 0.1049 - acc: 0.9588 - recall: 0.9573 - precision: 0.9604\n",
      "Epoch 00003: val_acc improved from 0.92826 to 0.92884, saving model to /content/gdrive/My Drive/Sapienza/NLP/HM1/Arci/../models/20190424__Final2.1.1(GPU)_all100_adam_lr0.0005_drLS0.4_drRec0.4_emb164_emb264_batch512_units256_maxl50_weights-chkp-03-0.93.hdf5\n",
      "336430/336430 [==============================] - 238s 706us/sample - loss: 0.1049 - acc: 0.9588 - recall: 0.9573 - precision: 0.9604 - val_loss: 0.2112 - val_acc: 0.9288 - val_recall: 0.9264 - val_precision: 0.9317\n",
      "Epoch 4/50\n",
      "336384/336430 [============================>.] - ETA: 0s - loss: 0.0881 - acc: 0.9654 - recall: 0.9644 - precision: 0.9666\n",
      "Epoch 00004: val_acc did not improve from 0.92884\n",
      "336430/336430 [==============================] - 226s 673us/sample - loss: 0.0881 - acc: 0.9654 - recall: 0.9644 - precision: 0.9666 - val_loss: 0.2285 - val_acc: 0.9270 - val_recall: 0.9252 - val_precision: 0.9293\n",
      "Epoch 5/50\n",
      "336384/336430 [============================>.] - ETA: 0s - loss: 0.0733 - acc: 0.9716 - recall: 0.9708 - precision: 0.9724\n",
      "Epoch 00005: val_acc did not improve from 0.92884\n",
      "336430/336430 [==============================] - 224s 665us/sample - loss: 0.0733 - acc: 0.9716 - recall: 0.9708 - precision: 0.9724 - val_loss: 0.2430 - val_acc: 0.9268 - val_recall: 0.9253 - val_precision: 0.9287\n",
      "Epoch 6/50\n",
      "336384/336430 [============================>.] - ETA: 0s - loss: 0.0598 - acc: 0.9773 - recall: 0.9768 - precision: 0.9778\n",
      "Epoch 00006: val_acc did not improve from 0.92884\n",
      "336430/336430 [==============================] - 224s 664us/sample - loss: 0.0598 - acc: 0.9773 - recall: 0.9768 - precision: 0.9778 - val_loss: 0.2658 - val_acc: 0.9252 - val_recall: 0.9241 - val_precision: 0.9267\n",
      "Epoch 7/50\n",
      "336384/336430 [============================>.] - ETA: 0s - loss: 0.0481 - acc: 0.9821 - recall: 0.9818 - precision: 0.9825\n",
      "Epoch 00007: val_acc did not improve from 0.92884\n",
      "336430/336430 [==============================] - 223s 662us/sample - loss: 0.0481 - acc: 0.9821 - recall: 0.9818 - precision: 0.9825 - val_loss: 0.3016 - val_acc: 0.9234 - val_recall: 0.9226 - val_precision: 0.9246\n",
      "Epoch 8/50\n",
      "336384/336430 [============================>.] - ETA: 0s - loss: 0.0385 - acc: 0.9859 - recall: 0.9857 - precision: 0.9862\n",
      "Epoch 00008: val_acc did not improve from 0.92884\n",
      "336430/336430 [==============================] - 223s 663us/sample - loss: 0.0385 - acc: 0.9859 - recall: 0.9857 - precision: 0.9862 - val_loss: 0.3235 - val_acc: 0.9232 - val_recall: 0.9226 - val_precision: 0.9242\n",
      "Epoch 9/50\n",
      "336384/336430 [============================>.] - ETA: 0s - loss: 0.0307 - acc: 0.9889 - recall: 0.9887 - precision: 0.9891\n",
      "Epoch 00009: val_acc did not improve from 0.92884\n",
      "336430/336430 [==============================] - 223s 664us/sample - loss: 0.0307 - acc: 0.9889 - recall: 0.9887 - precision: 0.9891 - val_loss: 0.3501 - val_acc: 0.9221 - val_recall: 0.9215 - val_precision: 0.9229\n",
      "Epoch 10/50\n",
      "336384/336430 [============================>.] - ETA: 0s - loss: 0.0249 - acc: 0.9911 - recall: 0.9910 - precision: 0.9912\n",
      "Epoch 00010: val_acc did not improve from 0.92884\n",
      "336430/336430 [==============================] - 223s 663us/sample - loss: 0.0249 - acc: 0.9911 - recall: 0.9910 - precision: 0.9912 - val_loss: 0.3847 - val_acc: 0.9220 - val_recall: 0.9215 - val_precision: 0.9227\n",
      "Epoch 11/50\n",
      "336384/336430 [============================>.] - ETA: 0s - loss: 0.0204 - acc: 0.9928 - recall: 0.9927 - precision: 0.9929\n",
      "Epoch 00011: val_acc did not improve from 0.92884\n",
      "336430/336430 [==============================] - 221s 657us/sample - loss: 0.0204 - acc: 0.9928 - recall: 0.9927 - precision: 0.9929 - val_loss: 0.4083 - val_acc: 0.9216 - val_recall: 0.9212 - val_precision: 0.9222\n",
      "Epoch 12/50\n",
      "336384/336430 [============================>.] - ETA: 0s - loss: 0.0171 - acc: 0.9940 - recall: 0.9939 - precision: 0.9941\n",
      "Epoch 00012: val_acc did not improve from 0.92884\n",
      "336430/336430 [==============================] - 222s 660us/sample - loss: 0.0171 - acc: 0.9940 - recall: 0.9939 - precision: 0.9941 - val_loss: 0.4346 - val_acc: 0.9210 - val_recall: 0.9206 - val_precision: 0.9215\n",
      "Epoch 13/50\n",
      "336384/336430 [============================>.] - ETA: 0s - loss: 0.0143 - acc: 0.9950 - recall: 0.9950 - precision: 0.9951\n",
      "Epoch 00013: val_acc did not improve from 0.92884\n",
      "336430/336430 [==============================] - 223s 663us/sample - loss: 0.0143 - acc: 0.9950 - recall: 0.9950 - precision: 0.9951 - val_loss: 0.4572 - val_acc: 0.9204 - val_recall: 0.9201 - val_precision: 0.9208\n",
      "Epoch 14/50\n",
      "336384/336430 [============================>.] - ETA: 0s - loss: 0.0122 - acc: 0.9957 - recall: 0.9957 - precision: 0.9958\n",
      "Epoch 00014: val_acc did not improve from 0.92884\n",
      "336430/336430 [==============================] - 222s 661us/sample - loss: 0.0122 - acc: 0.9957 - recall: 0.9957 - precision: 0.9958 - val_loss: 0.4906 - val_acc: 0.9202 - val_recall: 0.9199 - val_precision: 0.9206\n",
      "Epoch 15/50\n",
      "336384/336430 [============================>.] - ETA: 0s - loss: 0.0105 - acc: 0.9964 - recall: 0.9963 - precision: 0.9964\n",
      "Epoch 00015: val_acc did not improve from 0.92884\n",
      "336430/336430 [==============================] - 221s 657us/sample - loss: 0.0105 - acc: 0.9964 - recall: 0.9963 - precision: 0.9964 - val_loss: 0.5096 - val_acc: 0.9201 - val_recall: 0.9199 - val_precision: 0.9205\n",
      "Epoch 16/50\n",
      "336384/336430 [============================>.] - ETA: 0s - loss: 0.0092 - acc: 0.9968 - recall: 0.9968 - precision: 0.9969\n",
      "Epoch 00016: val_acc did not improve from 0.92884\n",
      "336430/336430 [==============================] - 221s 658us/sample - loss: 0.0092 - acc: 0.9968 - recall: 0.9968 - precision: 0.9969 - val_loss: 0.5213 - val_acc: 0.9201 - val_recall: 0.9199 - val_precision: 0.9205\n",
      "Epoch 17/50\n",
      "336384/336430 [============================>.] - ETA: 0s - loss: 0.0080 - acc: 0.9972 - recall: 0.9972 - precision: 0.9973\n",
      "Epoch 00017: val_acc did not improve from 0.92884\n",
      "336430/336430 [==============================] - 223s 662us/sample - loss: 0.0080 - acc: 0.9972 - recall: 0.9972 - precision: 0.9973 - val_loss: 0.5424 - val_acc: 0.9198 - val_recall: 0.9196 - val_precision: 0.9200\n",
      "Epoch 18/50\n",
      "201728/336430 [================>.............] - ETA: 1:27 - loss: 0.0066 - acc: 0.9977 - recall: 0.9977 - precision: 0.9978"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f2bcc0a4a888>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m                          \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m                          \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtime_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcsv_logger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                         )\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m           validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Get outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3074\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3076\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "learn_rate=0.0005\n",
    "dropout_LSTM = 0.4\n",
    "dropout_REC = 0.4\n",
    "print(\"*********** Training started on: \", datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d - %H:%M:%S'))\n",
    "\n",
    "maxlength = 50\n",
    "tconfig = train_config(tag=\"_Final2.1.1(GPU)\", dataset='all100', lenvocab=len(vocab), opt='adam', lr=learn_rate, drop_lstm=dropout_LSTM, drop_rec=dropout_REC, emb_size1=64, emb_size2=64, batch_size=512, lstm_units=256,maxlength=50)\n",
    "print(\"Configuration: \")\n",
    "print(tconfig.__dict__)  \n",
    "model = get_model_custom(tconfig)\n",
    "#model = toTPU(model)\n",
    "model_name = get_model_name(tconfig)\n",
    "print(\"Model name: \" + model_name)\n",
    "#models[id].summary()\n",
    "tf_callback, time_callback, csv_logger, checkpoint = get_callbacks(model_name)\n",
    "start = time.time()\n",
    "hist = model.fit(xinput, yout, \n",
    "                         validation_data=(xtest,ytest),\n",
    "                         epochs=50,\n",
    "                         batch_size=tconfig.batch_size,\n",
    "                         callbacks=[tf_callback, time_callback, csv_logger, checkpoint],\n",
    "                        )\n",
    "\n",
    "end = time.time()-start\n",
    "print(\"*********** Training finished on: \", datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d - %H:%M:%S'))\n",
    "print(\"Elapsed time: \", str(end), \" seconds (\", str(end/60), \" minutes).\")\n",
    "H = hist\n",
    "TrainingUtils.save_model_results(model,hist,root_path,model_name)\n",
    "print(\"Done.\\n\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "ZZ3uH5Qbwu27"
   ],
   "name": "ChineseWordSegmentation - FinalModel (2 Machine)",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
