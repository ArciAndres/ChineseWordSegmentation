{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4HVu_d9STkL8"
   },
   "source": [
    "# GridSearch 1 (All)\n",
    "Perfoms training with small part of the merged datasets (30%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rYKChoi7itqE"
   },
   "source": [
    "After having perfomed the training with dataset MSR and PKU, we realized that the test could be done with any of the data in the datasets availabe, so we will train a model with a parte of the complete datasets and try to come up with a general network representation to predict the labels accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sQ26beqeiLpq",
    "outputId": "535bb3c5-e5a1-4cd8-bfc0-c7e513cd6b1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')\n",
    "#drive.mount(\"/content/gdrive\", force_remount=True)\n",
    "\n",
    "import sys\n",
    "root_path = '/content/gdrive/My Drive/HM1/'  #change dir to your project folder\n",
    "sys.path.insert(0, root_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qw6hLV6_jErz"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "from tensorflow.keras.layers import Input, Embedding, Bidirectional, LSTM, Dense, TimeDistributed, concatenate, Activation, Masking, Dropout\n",
    "from tensorflow.keras.models import load_model\n",
    "import pickle\n",
    "import os, datetime, time\n",
    "\n",
    "import ChinesePreprocess as CP\n",
    "import ModelConfiguration as ModelConfig\n",
    "import TrainingUtils\n",
    "from code_provided.score import score\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bc4qYBRJjbNL"
   },
   "source": [
    "# Preprocess Initialization\n",
    "For all the general cases here, the reordered datasets will be used, and the vocabulary according to each dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qYtFYBFvjw87"
   },
   "outputs": [],
   "source": [
    "# Model to TPU (Only run when TPU is activated)\n",
    "TPU_WORKER = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "def toTPU(mdl):\n",
    "  tpu_model = tf.contrib.tpu.keras_to_tpu_model(mdl,strategy=tf.contrib.tpu.TPUDistributionStrategy(tf.contrib.cluster_resolver.TPUClusterResolver(TPU_WORKER)))\n",
    "  return tpu_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cVuK-JEX66u2"
   },
   "source": [
    "The default dataset will be MSR, unless especified different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FQqpJpkij2jj"
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "models = {}\n",
    "modelstpu = {}\n",
    "hists = {}\n",
    "\n",
    "def load_defaults_saved_all():\n",
    "  with open(root_path + \"../data_load/xinput_0.3all.pkl\",'rb') as file:\n",
    "    xinput = pickle.load(file)\n",
    "  with open(root_path + \"../data_load/yout_0.3all.pkl\",'rb') as file:\n",
    "    yout = pickle.load(file)\n",
    "  with open(root_path + \"../vocabs/vocab_all.pkl\",'rb') as voc:\n",
    "    vocab = pickle.load(voc)\n",
    "  with open(root_path + \"../data_load/xtest_all.pkl\",'rb') as voc:\n",
    "    xtest = pickle.load(voc)\n",
    "  with open(root_path + \"../data_load/ytest_all.pkl\",'rb') as voc:\n",
    "    ytest = pickle.load(voc)\n",
    "    #print(\"Vocabulary loaded.\")\n",
    "  \n",
    "  return xinput, yout, xtest, ytest, vocab\n",
    "\n",
    "def load_defaults_saved_all30():\n",
    "  with open(root_path + \"../data_load/xinput_all_l30.pkl\",'rb') as file:\n",
    "    xinput = pickle.load(file)\n",
    "  with open(root_path + \"../data_load/yout_all_l30.pkl\",'rb') as file:\n",
    "    yout = pickle.load(file)\n",
    "  with open(root_path + \"../vocabs/vocab_all.pkl\",'rb') as voc:\n",
    "    vocab = pickle.load(voc)\n",
    "  with open(root_path + \"../data_load/xtest_all_l30.pkl\",'rb') as voc:\n",
    "    xtest = pickle.load(voc)\n",
    "  with open(root_path + \"../data_load/ytest_all_l30.pkl\",'rb') as voc:\n",
    "    ytest = pickle.load(voc)\n",
    "    #print(\"Vocabulary loaded.\")\n",
    "  \n",
    "  return xinput, yout, xtest, ytest, vocab\n",
    "#-------------------------------------------\n",
    "########### Callbacks when fitting #########\n",
    "#-------------------------------------------\n",
    "\n",
    "def get_callbacks(model_name):\n",
    "  tf_callback = K.callbacks.TensorBoard(log_dir=root_path+'logs/'+model_name)\n",
    "  time_callback = TrainingUtils.TimeHistory()\n",
    "  csv_logger = K.callbacks.CSVLogger(root_path+'../models/'+model_name+'.log')\n",
    "  #checkpoint = K.callbacks.ModelCheckpoint(root_path+'../models/'+model_name+\"_weights-chkp-{epoch:02d}-{val_acc:.2f}.hdf5\", monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "  #earlyStop_callback= K.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None, restore_best_weights=False)\n",
    "\n",
    "  return tf_callback, time_callback, csv_logger#, checkpoint#, earlyStop_callback\n",
    "\n",
    "class train_config():\n",
    "  def __init__(self,tag,dataset,lenvocab,opt='adam', lr=0.001, drop_lstm=0.0, drop_rec=0.0, emb_size1=64, emb_size2=64, batch_size=256, lstm_units=256,maxlength=50):\n",
    "    self.tag=tag\n",
    "    self.dataset=dataset\n",
    "    self.lenvocab=lenvocab\n",
    "    self.opt=opt\n",
    "    self.lr=lr\n",
    "    self.drop_lstm=drop_lstm\n",
    "    self.drop_rec=drop_rec\n",
    "    self.emb_size1=emb_size1\n",
    "    self.emb_size2=emb_size2\n",
    "    self.batch_size=batch_size\n",
    "    self.lstm_units=lstm_units\n",
    "    self.maxlength=maxlength\n",
    "\n",
    "  \n",
    "def get_model_custom(TC): ## TC = Training configuration\n",
    "    \n",
    "    input1 =  Input(shape=(TC.maxlength,))\n",
    "    uni_layer = Embedding(TC.lenvocab,TC.emb_size1, input_length=TC.maxlength, mask_zero=True)(input1)\n",
    "    input2   = Input(shape=(TC.maxlength,))\n",
    "    bi_layer = Embedding(TC.lenvocab,TC.emb_size2, input_length=TC.maxlength, mask_zero=True)(input2)\n",
    "    ngram_layer = concatenate([uni_layer, bi_layer])\n",
    "    lstm_layer = Bidirectional(LSTM(TC.lstm_units, return_sequences=True, dropout=TC.drop_lstm, recurrent_dropout=TC.drop_rec))(ngram_layer)\n",
    "    time_dist_layer = TimeDistributed(Dense(4, activation='softmax'))(lstm_layer)\n",
    "    model = K.models.Model([input1,input2],time_dist_layer)\n",
    "    \n",
    "    if tconfig.opt=='adam':\n",
    "      optim = K.optimizers.Adam(lr=tconfig.lr)\n",
    "    else:\n",
    "      optim = K.optimizers.Adam(lr=tconfig.lr)\n",
    "    \n",
    "    model.compile(optimizer=optim, loss='categorical_crossentropy', metrics = ['acc', tf.keras.metrics.Recall(), tf.keras.metrics.Precision()])\n",
    "    return model\n",
    "  \n",
    "def get_model_name(tc):\n",
    "  model_name = datetime.datetime.fromtimestamp(time.time()).strftime('%Y%m%d_')+\"%s_%s_%s_lr%s_drLS%s_drRec%s_emb1%s_emb2%s_batch%s_units%s_maxl%s\" % (str(tc.tag),str(tc.dataset), tc.opt, str(tc.lr), str(tc.drop_lstm), str(tc.drop_rec), str(tc.emb_size1), str(tc.emb_size2), str(tc.batch_size), str(tc.lstm_units), str(tc.maxlength) )\n",
    "  return model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZZ3uH5Qbwu27"
   },
   "source": [
    "## Preprocessing all. Save useful variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9I68vx2WocN3"
   },
   "outputs": [],
   "source": [
    "def load_defaults_all(trainds=\"ALL_training_simp_reordered_shuf_30\", testds=\"ALL_test_gold_simp_reordered_shuf_30\", maxlength = 50, vocab_path = \"vocab_all\", samples_porcent =  1, verbose=False):\n",
    "  print(\"Loading configurations.\")\n",
    "  print(\"Training dataset: \", trainds)\n",
    "  print(\"Testing dataset: \", testds)\n",
    "  print(\"Vocabulary: \", vocab_path)\n",
    "  print(\"maxlength: \", maxlength)\n",
    "  \n",
    "  print(\"\\n******************** Loading vocabulary **********************\\n\")\n",
    "  with open(root_path + \"../vocabs/%s.pkl\" % vocab_path,'rb') as voc:\n",
    "    vocab = pickle.load(voc)\n",
    "    print(\"Vocabulary loaded.\")\n",
    "  \n",
    "  print(\"\\n******************** Extract train data (ALL) **********************\\n\")\n",
    "  ds_train = CP.ChinesePreprocess(root_path + \"../dataset/icwb2-data/training/%s.utf8\" % trainds, \n",
    "                                  num_samples=0, # Should be zero mostly. Becasue it's convenient to read everything most of the times. If not zero, some words to build the vocabulary might get lost in the trimming\n",
    "                                  vocabulary = vocab, # Static preset vocabulary. Same in training\n",
    "                                  verbose=False)\n",
    "\n",
    "  print(\"\\n\\n******************** Extract test data (ALL) *******************\\n\")\n",
    "  ds_test = CP.ChinesePreprocess(root_path + \"../dataset/icwb2-data/gold/%s.utf8\" % testds, \n",
    "                                 num_samples=0, \n",
    "                                 vocabulary = vocab,# Same vocabulary of training\n",
    "                                 verbose=False)\n",
    "\n",
    "    # Maximum length for padding\n",
    "  train = CP.ChinesePreprocess.apply_padding_data_and_labels(ds_train, maxlength, False)\n",
    "  test = CP.ChinesePreprocess.apply_padding_data_and_labels(ds_test, maxlength, False)\n",
    "  \n",
    "   # 100% of the samples\n",
    "  print(\"Percentage taken: \", samples_porcent*100,'%')\n",
    "  \n",
    "  # This is for trainings only. Not for testing \n",
    "  k = round(len(train.unigrams_pad)*samples_porcent)\n",
    "  print(\"Number of samples: \", k)\n",
    "  xinput = [train.unigrams_pad[:k], train.bigrams_pad[:k]]\n",
    "  yout = train.labels[:k]\n",
    "  print(\"Done.\")\n",
    "  return train, test, xinput, yout, vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P3yCnNVmoht5"
   },
   "outputs": [],
   "source": [
    "#train, test, xinput, yout, vocab = load_defaults_all(maxlength=30,samples_porcent=0.3)\n",
    "#with open(root_path+\"../data_load/xinput_all_l30.pkl\", 'wb') as file:\n",
    "#  pickle.dump(xinput, file)\n",
    "#with open(root_path+\"../data_load/yout_all_l30.pkl\", 'wb') as file:\n",
    "#  pickle.dump(yout, file)\n",
    "#with open(root_path+\"../data_load/xtest_all_l30.pkl\", 'wb') as file:\n",
    "#  pickle.dump([test.unigrams_pad, test.bigrams_pad], file)\n",
    "#with open(root_path+\"../data_load/ytest_all_l30.pkl\", 'wb') as file:\n",
    "#  pickle.dump(test.labels, file)\n",
    "#with open(root_path+\"../data_load/vocab_all.pkl\", 'wb') as file:\n",
    "#  pickle.dump(vocab, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "-PLM7nLElpVj",
    "outputId": "2b951991-ba4b-4449-9306-863755de0cc2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([907116, 124420, 333878, 955007,  79251, 879095, 843694, 907757,\n",
       "       637562, 127326, 869954, 188286, 187659, 333878, 530873, 569864,\n",
       "       869954, 992885, 188286, 217176, 897331, 217176, 955007, 204905,\n",
       "       992885, 188286, 333878,  79251, 702761, 835892,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0,      0,      0,      0,      0,      0,      0,\n",
       "            0,      0], dtype=int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bi4NSKUeoVnn"
   },
   "source": [
    "# Grid search 1 (All)\n",
    "Will iterate learning rate and Dropout (LSTM), in the range of LR = [0.001,0.002,0.004] and Drop_LSTM = [0,0.1,0.4,0.6].\n",
    "Other values will remain constant: Batch=512, Drop_Rec=0, Emb=64, Units=256\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bv39QAOezk4R"
   },
   "source": [
    "## Training 1.1.1: LR:0.001, Drop_LSTM=0.0\n",
    "|   Static   | Variable                    |\n",
    "|:----------:|-----------------------------|\n",
    "| LR=0.001   | Drop_LSTM = [0,0.1,0.4,0.6] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oy9TjEcUzkGq"
   },
   "outputs": [],
   "source": [
    "xinput, yout, xtest, ytest, vocab = load_defaults_saved_all()\n",
    "#x test is a list of unigram and bigrams like this [unigrams, bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1122
    },
    "colab_type": "code",
    "id": "hDitdIYfzjQN",
    "outputId": "52bf3f9c-ab1e-4ada-884c-68b89c001a21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** Training started on:  2019-04-24 - 01:09:55\n",
      "Configuration: \n",
      "{'tag': '_GS1.1.1', 'dataset': 'msrpku', 'lenvocab': 1043604, 'opt': 'adam', 'lr': 0.001, 'drop_lstm': 0, 'drop_rec': 0.0, 'emb_size1': 64, 'emb_size2': 64, 'batch_size': 512, 'lstm_units': 256, 'maxlength': 50}\n",
      "Model name: 20190424__GS1.1.1_msrpku_adam_lr0.001_drLS0_drRec0.0_emb164_emb264_batch512_units256_maxl50\n",
      "Train on 100929 samples, validate on 12590 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "100929/100929 [==============================] - 69s 687us/sample - loss: 0.5004 - acc: 0.7980 - recall_3: 0.6806 - precision_3: 0.9174 - val_loss: 0.2365 - val_acc: 0.9125 - val_recall_3: 0.9053 - val_precision_3: 0.9200\n",
      "Epoch 2/20\n",
      "100929/100929 [==============================] - 66s 656us/sample - loss: 0.1364 - acc: 0.9483 - recall_3: 0.9453 - precision_3: 0.9516 - val_loss: 0.2257 - val_acc: 0.9194 - val_recall_3: 0.9151 - val_precision_3: 0.9242\n",
      "Epoch 3/20\n",
      "100929/100929 [==============================] - 66s 659us/sample - loss: 0.0885 - acc: 0.9661 - recall_3: 0.9649 - precision_3: 0.9673 - val_loss: 0.2491 - val_acc: 0.9192 - val_recall_3: 0.9166 - val_precision_3: 0.9225\n",
      "Epoch 4/20\n",
      "100929/100929 [==============================] - 66s 654us/sample - loss: 0.0575 - acc: 0.9787 - recall_3: 0.9781 - precision_3: 0.9793 - val_loss: 0.2920 - val_acc: 0.9167 - val_recall_3: 0.9151 - val_precision_3: 0.9191\n",
      "Epoch 5/20\n",
      "100929/100929 [==============================] - 65s 643us/sample - loss: 0.0346 - acc: 0.9878 - recall_3: 0.9876 - precision_3: 0.9881 - val_loss: 0.3939 - val_acc: 0.9086 - val_recall_3: 0.9077 - val_precision_3: 0.9100\n",
      "Epoch 6/20\n",
      "100929/100929 [==============================] - 67s 660us/sample - loss: 0.0195 - acc: 0.9936 - recall_3: 0.9935 - precision_3: 0.9937 - val_loss: 0.4170 - val_acc: 0.9122 - val_recall_3: 0.9115 - val_precision_3: 0.9133\n",
      "Epoch 7/20\n",
      "100929/100929 [==============================] - 66s 653us/sample - loss: 0.0112 - acc: 0.9966 - recall_3: 0.9966 - precision_3: 0.9967 - val_loss: 0.4791 - val_acc: 0.9108 - val_recall_3: 0.9103 - val_precision_3: 0.9117\n",
      "Epoch 8/20\n",
      "100929/100929 [==============================] - 65s 645us/sample - loss: 0.0063 - acc: 0.9982 - recall_3: 0.9982 - precision_3: 0.9982 - val_loss: 0.5419 - val_acc: 0.9082 - val_recall_3: 0.9078 - val_precision_3: 0.9089\n",
      "Epoch 9/20\n",
      "100929/100929 [==============================] - 65s 649us/sample - loss: 0.0038 - acc: 0.9990 - recall_3: 0.9990 - precision_3: 0.9990 - val_loss: 0.5738 - val_acc: 0.9085 - val_recall_3: 0.9081 - val_precision_3: 0.9091\n",
      "Epoch 10/20\n",
      "100929/100929 [==============================] - 64s 637us/sample - loss: 0.0023 - acc: 0.9995 - recall_3: 0.9995 - precision_3: 0.9995 - val_loss: 0.5920 - val_acc: 0.9097 - val_recall_3: 0.9093 - val_precision_3: 0.9102\n",
      "Epoch 11/20\n",
      "100929/100929 [==============================] - 66s 650us/sample - loss: 0.0015 - acc: 0.9997 - recall_3: 0.9997 - precision_3: 0.9997 - val_loss: 0.6180 - val_acc: 0.9088 - val_recall_3: 0.9085 - val_precision_3: 0.9093\n",
      "Epoch 12/20\n",
      "100929/100929 [==============================] - 66s 650us/sample - loss: 0.0011 - acc: 0.9998 - recall_3: 0.9998 - precision_3: 0.9998 - val_loss: 0.6386 - val_acc: 0.9089 - val_recall_3: 0.9086 - val_precision_3: 0.9094\n",
      "Epoch 13/20\n",
      "100929/100929 [==============================] - 65s 646us/sample - loss: 8.0517e-04 - acc: 0.9999 - recall_3: 0.9999 - precision_3: 0.9999 - val_loss: 0.6812 - val_acc: 0.9062 - val_recall_3: 0.9059 - val_precision_3: 0.9066\n",
      "Epoch 14/20\n",
      "100929/100929 [==============================] - 65s 645us/sample - loss: 6.8985e-04 - acc: 0.9999 - recall_3: 0.9999 - precision_3: 0.9999 - val_loss: 0.6944 - val_acc: 0.9066 - val_recall_3: 0.9064 - val_precision_3: 0.9070\n",
      "Epoch 15/20\n",
      "100929/100929 [==============================] - 66s 650us/sample - loss: 9.9171e-04 - acc: 0.9998 - recall_3: 0.9998 - precision_3: 0.9998 - val_loss: 0.6913 - val_acc: 0.9073 - val_recall_3: 0.9070 - val_precision_3: 0.9077\n",
      "Epoch 16/20\n",
      "100929/100929 [==============================] - 64s 638us/sample - loss: 0.0014 - acc: 0.9997 - recall_3: 0.9997 - precision_3: 0.9997 - val_loss: 0.6844 - val_acc: 0.9085 - val_recall_3: 0.9083 - val_precision_3: 0.9089\n",
      "Epoch 17/20\n",
      "100929/100929 [==============================] - 66s 650us/sample - loss: 0.0026 - acc: 0.9993 - recall_3: 0.9993 - precision_3: 0.9993 - val_loss: 0.6840 - val_acc: 0.9077 - val_recall_3: 0.9075 - val_precision_3: 0.9081\n",
      "Epoch 18/20\n",
      "100929/100929 [==============================] - 65s 646us/sample - loss: 0.0048 - acc: 0.9985 - recall_3: 0.9985 - precision_3: 0.9985 - val_loss: 0.6565 - val_acc: 0.9067 - val_recall_3: 0.9065 - val_precision_3: 0.9072\n",
      "Epoch 19/20\n",
      "100929/100929 [==============================] - 65s 647us/sample - loss: 0.0066 - acc: 0.9979 - recall_3: 0.9979 - precision_3: 0.9979 - val_loss: 0.6374 - val_acc: 0.9080 - val_recall_3: 0.9077 - val_precision_3: 0.9085\n",
      "Epoch 20/20\n",
      "100929/100929 [==============================] - 66s 656us/sample - loss: 0.0050 - acc: 0.9985 - recall_3: 0.9984 - precision_3: 0.9985 - val_loss: 0.6570 - val_acc: 0.9072 - val_recall_3: 0.9069 - val_precision_3: 0.9077\n",
      "*********** Training finished on:  2019-04-24 - 01:32:00\n",
      "Elapsed time:  1323.48686170578  seconds ( 22.058114361763  minutes).\n",
      "[INFO] Saving full model...\n",
      "[INFO] Model saved successfully.\n",
      "\n",
      "[INFO] Saving model plot...\n",
      "[INFO] Model plot saved.\n",
      "\n",
      "[INFO] Saving model weights...\n",
      "[INFO] Model weights saved successfully.\n",
      "\n",
      "[INFO] Saving history of the training...\n",
      "[INFO] History of the training successfully.\n",
      "\n",
      "Done.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn_rate=0.001\n",
    "dropout_LSTM = 0\n",
    "print(\"*********** Training started on: \", datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d - %H:%M:%S'))\n",
    "\n",
    "maxlength = 50\n",
    "tconfig = ModelConfig.train_config(tag=\"_GSA1.1.1\", dataset='msrpku', lenvocab=len(vocab), opt='adam', lr=learn_rate, drop_lstm=dropout_LSTM, drop_rec=0.0, emb_size1=64, emb_size2=64, batch_size=512, lstm_units=256,maxlength=50)\n",
    "print(\"Configuration: \")\n",
    "print(tconfig.__dict__)  \n",
    "model = ModelConfig.get_model_custom(tconfig)\n",
    "#model = toTPU(model)\n",
    "model_name = ModelConfig.get_model_name(tconfig)\n",
    "print(\"Model name: \" + model_name)\n",
    "#models[id].summary()\n",
    "tf_callback, time_callback, csv_logger = get_callbacks(model_name)\n",
    "start = time.time()\n",
    "hist = model.fit(xinput, yout, \n",
    "                         validation_data=(xtest,ytest),\n",
    "                         epochs=20,\n",
    "                         batch_size=tconfig.batch_size,\n",
    "                         callbacks=[tf_callback, time_callback, csv_logger],\n",
    "                         shuffle=True\n",
    "                        )\n",
    "\n",
    "end = time.time()-start\n",
    "print(\"*********** Training finished on: \", datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d - %H:%M:%S'))\n",
    "print(\"Elapsed time: \", str(end), \" seconds (\", str(end/60), \" minutes).\")\n",
    "H = hist\n",
    "hi = H.history\n",
    "hi['time_start'] = time_callback.epoch_time_start \n",
    "hi['elapsed'] = time_callback.times\n",
    "TrainingUtils.save_model_results(model,hist,root_path,model_name)\n",
    "print(\"Done.\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zOiQu9U6_ejX"
   },
   "source": [
    "## Training 1.1.2: LR:0.001, Drop_LSTM=0.1\n",
    "|   Static   | Variable                    |\n",
    "|:----------:|-----------------------------|\n",
    "| LR=0.001   | Drop_LSTM = [0,0.1,0.4,0.6] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t3ajEjpg_ejY"
   },
   "outputs": [],
   "source": [
    "xinput, yout, xtest, ytest, vocab = load_defaults_saved_all()\n",
    "#x test is a list of unigram and bigrams like this [unigrams, bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1822
    },
    "colab_type": "code",
    "id": "S7N10Diq_ejb",
    "outputId": "32cf1e25-731a-44da-fe23-6e7b6fd70404"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** Training started on:  2019-04-24 - 01:42:28\n",
      "Configuration: \n",
      "{'tag': '_GSA1.1.2', 'dataset': 'all0.3', 'lenvocab': 1043604, 'opt': 'adam', 'lr': 0.001, 'drop_lstm': 0.1, 'drop_rec': 0.0, 'emb_size1': 64, 'emb_size2': 64, 'batch_size': 512, 'lstm_units': 256, 'maxlength': 50}\n",
      "Model name: 20190424__GSA1.1.2_all0.3_adam_lr0.001_drLS0.1_drRec0.0_emb164_emb264_batch512_units256_maxl50\n",
      "Train on 100929 samples, validate on 12590 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "100864/100929 [============================>.] - ETA: 0s - loss: 0.5210 - acc: 0.7933 - recall_1: 0.6632 - precision_1: 0.9162\n",
      "Epoch 00001: val_acc improved from -inf to 0.91275, saving model to /content/gdrive/My Drive/MasterSapienza/Semestre2/NLP/HM1/Arci/../models/20190424__GSA1.1.2_all0.3_adam_lr0.001_drLS0.1_drRec0.0_emb164_emb264_batch512_units256_maxl50_weights-chkp-01-0.91.hdf5\n",
      "100929/100929 [==============================] - 81s 806us/sample - loss: 0.5207 - acc: 0.7934 - recall_1: 0.6633 - precision_1: 0.9162 - val_loss: 0.2370 - val_acc: 0.9127 - val_recall_1: 0.9052 - val_precision_1: 0.9217\n",
      "Epoch 2/20\n",
      "100864/100929 [============================>.] - ETA: 0s - loss: 0.1398 - acc: 0.9473 - recall_1: 0.9441 - precision_1: 0.9507\n",
      "Epoch 00002: val_acc improved from 0.91275 to 0.91945, saving model to /content/gdrive/My Drive/MasterSapienza/Semestre2/NLP/HM1/Arci/../models/20190424__GSA1.1.2_all0.3_adam_lr0.001_drLS0.1_drRec0.0_emb164_emb264_batch512_units256_maxl50_weights-chkp-02-0.92.hdf5\n",
      "100929/100929 [==============================] - 81s 799us/sample - loss: 0.1398 - acc: 0.9473 - recall_1: 0.9441 - precision_1: 0.9507 - val_loss: 0.2258 - val_acc: 0.9195 - val_recall_1: 0.9149 - val_precision_1: 0.9249\n",
      "Epoch 3/20\n",
      "100864/100929 [============================>.] - ETA: 0s - loss: 0.0932 - acc: 0.9643 - recall_1: 0.9630 - precision_1: 0.9656\n",
      "Epoch 00003: val_acc did not improve from 0.91945\n",
      "100929/100929 [==============================] - 70s 698us/sample - loss: 0.0932 - acc: 0.9643 - recall_1: 0.9630 - precision_1: 0.9656 - val_loss: 0.2493 - val_acc: 0.9174 - val_recall_1: 0.9145 - val_precision_1: 0.9212\n",
      "Epoch 4/20\n",
      "100864/100929 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9762 - recall_1: 0.9755 - precision_1: 0.9769\n",
      "Epoch 00004: val_acc did not improve from 0.91945\n",
      "100929/100929 [==============================] - 69s 683us/sample - loss: 0.0635 - acc: 0.9762 - recall_1: 0.9755 - precision_1: 0.9769 - val_loss: 0.2874 - val_acc: 0.9165 - val_recall_1: 0.9145 - val_precision_1: 0.9191\n",
      "Epoch 5/20\n",
      "100864/100929 [============================>.] - ETA: 0s - loss: 0.0412 - acc: 0.9852 - recall_1: 0.9849 - precision_1: 0.9856\n",
      "Epoch 00005: val_acc did not improve from 0.91945\n",
      "100929/100929 [==============================] - 68s 676us/sample - loss: 0.0412 - acc: 0.9852 - recall_1: 0.9849 - precision_1: 0.9856 - val_loss: 0.3511 - val_acc: 0.9129 - val_recall_1: 0.9117 - val_precision_1: 0.9145\n",
      "Epoch 6/20\n",
      "100864/100929 [============================>.] - ETA: 0s - loss: 0.0261 - acc: 0.9910 - recall_1: 0.9909 - precision_1: 0.9912\n",
      "Epoch 00006: val_acc did not improve from 0.91945\n",
      "100929/100929 [==============================] - 67s 668us/sample - loss: 0.0261 - acc: 0.9910 - recall_1: 0.9909 - precision_1: 0.9912 - val_loss: 0.3980 - val_acc: 0.9117 - val_recall_1: 0.9109 - val_precision_1: 0.9130\n",
      "Epoch 7/20\n",
      "100864/100929 [============================>.] - ETA: 0s - loss: 0.0165 - acc: 0.9946 - recall_1: 0.9945 - precision_1: 0.9947\n",
      "Epoch 00007: val_acc did not improve from 0.91945\n",
      "100929/100929 [==============================] - 67s 666us/sample - loss: 0.0165 - acc: 0.9946 - recall_1: 0.9945 - precision_1: 0.9947 - val_loss: 0.4594 - val_acc: 0.9107 - val_recall_1: 0.9102 - val_precision_1: 0.9117\n",
      "Epoch 8/20\n",
      "100864/100929 [============================>.] - ETA: 0s - loss: 0.0106 - acc: 0.9967 - recall_1: 0.9966 - precision_1: 0.9967\n",
      "Epoch 00008: val_acc did not improve from 0.91945\n",
      "100929/100929 [==============================] - 67s 667us/sample - loss: 0.0106 - acc: 0.9967 - recall_1: 0.9966 - precision_1: 0.9967 - val_loss: 0.5003 - val_acc: 0.9086 - val_recall_1: 0.9080 - val_precision_1: 0.9094\n",
      "Epoch 9/20\n",
      "100864/100929 [============================>.] - ETA: 0s - loss: 0.0070 - acc: 0.9979 - recall_1: 0.9979 - precision_1: 0.9979\n",
      "Epoch 00009: val_acc did not improve from 0.91945\n",
      "100929/100929 [==============================] - 68s 676us/sample - loss: 0.0070 - acc: 0.9979 - recall_1: 0.9979 - precision_1: 0.9979 - val_loss: 0.5401 - val_acc: 0.9088 - val_recall_1: 0.9084 - val_precision_1: 0.9095\n",
      "Epoch 10/20\n",
      "100864/100929 [============================>.] - ETA: 0s - loss: 0.0049 - acc: 0.9986 - recall_1: 0.9986 - precision_1: 0.9986\n",
      "Epoch 00010: val_acc did not improve from 0.91945\n",
      "100929/100929 [==============================] - 67s 664us/sample - loss: 0.0049 - acc: 0.9986 - recall_1: 0.9986 - precision_1: 0.9986 - val_loss: 0.5832 - val_acc: 0.9069 - val_recall_1: 0.9066 - val_precision_1: 0.9075\n",
      "Epoch 11/20\n",
      "100864/100929 [============================>.] - ETA: 0s - loss: 0.0036 - acc: 0.9990 - recall_1: 0.9990 - precision_1: 0.9990\n",
      "Epoch 00011: val_acc did not improve from 0.91945\n",
      "100929/100929 [==============================] - 67s 666us/sample - loss: 0.0036 - acc: 0.9990 - recall_1: 0.9990 - precision_1: 0.9990 - val_loss: 0.6107 - val_acc: 0.9073 - val_recall_1: 0.9071 - val_precision_1: 0.9079\n",
      "Epoch 12/20\n",
      "100864/100929 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9993 - recall_1: 0.9993 - precision_1: 0.9993\n",
      "Epoch 00012: val_acc did not improve from 0.91945\n",
      "100929/100929 [==============================] - 67s 665us/sample - loss: 0.0028 - acc: 0.9993 - recall_1: 0.9993 - precision_1: 0.9993 - val_loss: 0.6251 - val_acc: 0.9076 - val_recall_1: 0.9073 - val_precision_1: 0.9082\n",
      "Epoch 13/20\n",
      "100864/100929 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9994 - recall_1: 0.9994 - precision_1: 0.9994\n",
      "Epoch 00013: val_acc did not improve from 0.91945\n",
      "100929/100929 [==============================] - 68s 672us/sample - loss: 0.0024 - acc: 0.9994 - recall_1: 0.9994 - precision_1: 0.9994 - val_loss: 0.6567 - val_acc: 0.9077 - val_recall_1: 0.9075 - val_precision_1: 0.9081\n",
      "Epoch 14/20\n",
      "100864/100929 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9994 - recall_1: 0.9994 - precision_1: 0.9994\n",
      "Epoch 00014: val_acc did not improve from 0.91945\n",
      "100929/100929 [==============================] - 68s 671us/sample - loss: 0.0021 - acc: 0.9994 - recall_1: 0.9994 - precision_1: 0.9994 - val_loss: 0.6735 - val_acc: 0.9075 - val_recall_1: 0.9073 - val_precision_1: 0.9079\n",
      "Epoch 15/20\n",
      "100864/100929 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9995 - recall_1: 0.9995 - precision_1: 0.9995\n",
      "Epoch 00015: val_acc did not improve from 0.91945\n",
      "100929/100929 [==============================] - 67s 661us/sample - loss: 0.0020 - acc: 0.9995 - recall_1: 0.9995 - precision_1: 0.9995 - val_loss: 0.6794 - val_acc: 0.9074 - val_recall_1: 0.9071 - val_precision_1: 0.9077\n",
      "Epoch 16/20\n",
      "100864/100929 [============================>.] - ETA: 0s - loss: 0.0020 - acc: 0.9995 - recall_1: 0.9995 - precision_1: 0.9995\n",
      "Epoch 00016: val_acc did not improve from 0.91945\n",
      "100929/100929 [==============================] - 67s 662us/sample - loss: 0.0020 - acc: 0.9995 - recall_1: 0.9995 - precision_1: 0.9995 - val_loss: 0.6840 - val_acc: 0.9080 - val_recall_1: 0.9078 - val_precision_1: 0.9084\n",
      "Epoch 17/20\n",
      "100864/100929 [============================>.] - ETA: 0s - loss: 0.0021 - acc: 0.9994 - recall_1: 0.9994 - precision_1: 0.9994\n",
      "Epoch 00017: val_acc did not improve from 0.91945\n",
      "100929/100929 [==============================] - 67s 665us/sample - loss: 0.0021 - acc: 0.9994 - recall_1: 0.9994 - precision_1: 0.9994 - val_loss: 0.7075 - val_acc: 0.9061 - val_recall_1: 0.9060 - val_precision_1: 0.9066\n",
      "Epoch 18/20\n",
      "100864/100929 [============================>.] - ETA: 0s - loss: 0.0024 - acc: 0.9993 - recall_1: 0.9993 - precision_1: 0.9993\n",
      "Epoch 00018: val_acc did not improve from 0.91945\n",
      "100929/100929 [==============================] - 68s 677us/sample - loss: 0.0024 - acc: 0.9993 - recall_1: 0.9993 - precision_1: 0.9993 - val_loss: 0.6987 - val_acc: 0.9067 - val_recall_1: 0.9065 - val_precision_1: 0.9071\n",
      "Epoch 19/20\n",
      "100864/100929 [============================>.] - ETA: 0s - loss: 0.0026 - acc: 0.9992 - recall_1: 0.9992 - precision_1: 0.9992\n",
      "Epoch 00019: val_acc did not improve from 0.91945\n",
      "100929/100929 [==============================] - 67s 664us/sample - loss: 0.0026 - acc: 0.9992 - recall_1: 0.9992 - precision_1: 0.9992 - val_loss: 0.6966 - val_acc: 0.9066 - val_recall_1: 0.9064 - val_precision_1: 0.9070\n",
      "Epoch 20/20\n",
      "100864/100929 [============================>.] - ETA: 0s - loss: 0.0028 - acc: 0.9992 - recall_1: 0.9992 - precision_1: 0.9992\n",
      "Epoch 00020: val_acc did not improve from 0.91945\n",
      "100929/100929 [==============================] - 67s 664us/sample - loss: 0.0028 - acc: 0.9992 - recall_1: 0.9991 - precision_1: 0.9992 - val_loss: 0.7089 - val_acc: 0.9067 - val_recall_1: 0.9065 - val_precision_1: 0.9070\n",
      "*********** Training finished on:  2019-04-24 - 02:05:37\n",
      "Elapsed time:  1387.637565612793  seconds ( 23.127292760213216  minutes).\n",
      "[INFO] Saving full model...\n",
      "[INFO] Model saved successfully.\n",
      "\n",
      "[INFO] Saving model plot...\n",
      "[INFO] Model plot saved.\n",
      "\n",
      "[INFO] Saving model weights...\n",
      "[INFO] Model weights saved successfully.\n",
      "\n",
      "[INFO] Saving history of the training...\n",
      "[INFO] History of the training successfully.\n",
      "\n",
      "Done.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn_rate=0.001\n",
    "dropout_LSTM = 0.1\n",
    "print(\"*********** Training started on: \", datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d - %H:%M:%S'))\n",
    "\n",
    "maxlength = 50\n",
    "tconfig = ModelConfig.train_config(tag=\"_GSA1.1.2\", dataset='all0.3', lenvocab=len(vocab), opt='adam', lr=learn_rate, drop_lstm=dropout_LSTM, drop_rec=0.0, emb_size1=64, emb_size2=64, batch_size=512, lstm_units=256,maxlength=50)\n",
    "print(\"Configuration: \")\n",
    "print(tconfig.__dict__)  \n",
    "model = ModelConfig.get_model_custom(tconfig)\n",
    "#model = toTPU(model)\n",
    "model_name = ModelConfig.get_model_name(tconfig)\n",
    "print(\"Model name: \" + model_name)\n",
    "#models[id].summary()\n",
    "tf_callback, time_callback, csv_logger, checkpoint = get_callbacks(model_name)\n",
    "start = time.time()\n",
    "hist = model.fit(xinput, yout, \n",
    "                         validation_data=(xtest,ytest),\n",
    "                         epochs=20,\n",
    "                         batch_size=tconfig.batch_size,\n",
    "                         callbacks=[tf_callback, time_callback, csv_logger, checkpoint],\n",
    "                         shuffle=True\n",
    "                        )\n",
    "\n",
    "end = time.time()-start\n",
    "print(\"*********** Training finished on: \", datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d - %H:%M:%S'))\n",
    "print(\"Elapsed time: \", str(end), \" seconds (\", str(end/60), \" minutes).\")\n",
    "H = hist\n",
    "hi = H.history\n",
    "hi['time_start'] = time_callback.epoch_time_start \n",
    "hi['elapsed'] = time_callback.times\n",
    "TrainingUtils.save_model_results(model,hist,root_path,model_name)\n",
    "print(\"Done.\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-J_kSCYEGcu4"
   },
   "source": [
    "## Training 1.1.3: LR:0.001, Drop_LSTM=0.4\n",
    "|   Static   | Variable                    |\n",
    "|:----------:|-----------------------------|\n",
    "| LR=0.001   | Drop_LSTM = [0,0.1,0.4,0.6] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fFO5er7xGcu5"
   },
   "outputs": [],
   "source": [
    "xinput, yout, xtest, ytest, vocab = load_defaults_saved_all()\n",
    "#x test is a list of unigram and bigrams like this [unigrams, bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uKEkBIUXGcu-"
   },
   "outputs": [],
   "source": [
    "learn_rate=0.001\n",
    "dropout_LSTM = 0.4\n",
    "print(\"*********** Training started on: \", datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d - %H:%M:%S'))\n",
    "\n",
    "maxlength = 50\n",
    "tconfig = ModelConfig.train_config(tag=\"_GSA1.1.3\", dataset='all0.3', lenvocab=len(vocab), opt='adam', lr=learn_rate, drop_lstm=dropout_LSTM, drop_rec=0.0, emb_size1=64, emb_size2=64, batch_size=512, lstm_units=256,maxlength=50)\n",
    "print(\"Configuration: \")\n",
    "print(tconfig.__dict__)  \n",
    "model = ModelConfig.get_model_custom(tconfig)\n",
    "model = toTPU(model)\n",
    "model_name = ModelConfig.get_model_name(tconfig)\n",
    "print(\"Model name: \" + model_name)\n",
    "#models[id].summary()\n",
    "tf_callback, time_callback, csv_logger = get_callbacks(model_name)\n",
    "start = time.time()\n",
    "hist = model.fit(xinput, yout, \n",
    "                         validation_data=(xtest,ytest),\n",
    "                         epochs=20,\n",
    "                         batch_size=tconfig.batch_size,\n",
    "                         callbacks=[tf_callback, time_callback, csv_logger],\n",
    "                         shuffle=True\n",
    "                        )\n",
    "\n",
    "end = time.time()-start\n",
    "print(\"*********** Training finished on: \", datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d - %H:%M:%S'))\n",
    "print(\"Elapsed time: \", str(end), \" seconds (\", str(end/60), \" minutes).\")\n",
    "H = hist\n",
    "hi = H.history\n",
    "hi['time_start'] = time_callback.epoch_time_start \n",
    "hi['elapsed'] = time_callback.times\n",
    "TrainingUtils.save_model_results(model,hist,root_path,model_name)\n",
    "print(\"Done.\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vsRMWO-OSnpv"
   },
   "source": [
    "## Training 1.1.4: LR:0.001, Drop_LSTM=0.6\n",
    "|   Static   | Variable                    |\n",
    "|:----------:|-----------------------------|\n",
    "| LR=0.001   | Drop_LSTM = [0,0.1,0.4,0.6] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rg42e0QRSnpw"
   },
   "outputs": [],
   "source": [
    "xinput, yout, xtest, ytest, vocab = load_defaults_saved_all()\n",
    "#x test is a list of unigram and bigrams like this [unigrams, bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1122
    },
    "colab_type": "code",
    "id": "yFhgDZY-Snpy",
    "outputId": "fa08264e-020c-48a4-9f6f-14b805dad679"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** Training started on:  2019-04-24 - 03:07:01\n",
      "Configuration: \n",
      "{'tag': '_GSA1.1.4', 'dataset': 'all0.3', 'lenvocab': 1043604, 'opt': 'adam', 'lr': 0.001, 'drop_lstm': 0.6, 'drop_rec': 0.0, 'emb_size1': 64, 'emb_size2': 64, 'batch_size': 512, 'lstm_units': 256, 'maxlength': 50}\n",
      "Model name: 20190424__GSA1.1.4_all0.3_adam_lr0.001_drLS0.6_drRec0.0_emb164_emb264_batch512_units256_maxl50\n",
      "Train on 100929 samples, validate on 12590 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "100929/100929 [==============================] - 75s 743us/sample - loss: 0.5354 - acc: 0.7840 - recall_1: 0.6569 - precision_1: 0.9100 - val_loss: 0.2433 - val_acc: 0.9102 - val_recall_1: 0.9015 - val_precision_1: 0.9195\n",
      "Epoch 2/20\n",
      "100929/100929 [==============================] - 71s 708us/sample - loss: 0.1543 - acc: 0.9427 - recall_1: 0.9385 - precision_1: 0.9468 - val_loss: 0.2340 - val_acc: 0.9163 - val_recall_1: 0.9117 - val_precision_1: 0.9216\n",
      "Epoch 3/20\n",
      "100929/100929 [==============================] - 70s 697us/sample - loss: 0.1083 - acc: 0.9589 - recall_1: 0.9572 - precision_1: 0.9607 - val_loss: 0.2440 - val_acc: 0.9192 - val_recall_1: 0.9163 - val_precision_1: 0.9230\n",
      "Epoch 4/20\n",
      "100929/100929 [==============================] - 72s 709us/sample - loss: 0.0832 - acc: 0.9683 - recall_1: 0.9673 - precision_1: 0.9694 - val_loss: 0.2567 - val_acc: 0.9186 - val_recall_1: 0.9163 - val_precision_1: 0.9219\n",
      "Epoch 5/20\n",
      "100929/100929 [==============================] - 73s 719us/sample - loss: 0.0635 - acc: 0.9761 - recall_1: 0.9755 - precision_1: 0.9768 - val_loss: 0.2920 - val_acc: 0.9163 - val_recall_1: 0.9147 - val_precision_1: 0.9188\n",
      "Epoch 6/20\n",
      "100929/100929 [==============================] - 71s 707us/sample - loss: 0.0481 - acc: 0.9823 - recall_1: 0.9819 - precision_1: 0.9828 - val_loss: 0.3292 - val_acc: 0.9145 - val_recall_1: 0.9132 - val_precision_1: 0.9165\n",
      "Epoch 7/20\n",
      "100929/100929 [==============================] - 71s 706us/sample - loss: 0.0360 - acc: 0.9871 - recall_1: 0.9868 - precision_1: 0.9874 - val_loss: 0.3630 - val_acc: 0.9133 - val_recall_1: 0.9123 - val_precision_1: 0.9149\n",
      "Epoch 8/20\n",
      "100929/100929 [==============================] - 72s 710us/sample - loss: 0.0271 - acc: 0.9905 - recall_1: 0.9903 - precision_1: 0.9907 - val_loss: 0.4145 - val_acc: 0.9099 - val_recall_1: 0.9092 - val_precision_1: 0.9112\n",
      "Epoch 9/20\n",
      "100929/100929 [==============================] - 72s 709us/sample - loss: 0.0206 - acc: 0.9929 - recall_1: 0.9928 - precision_1: 0.9930 - val_loss: 0.4352 - val_acc: 0.9096 - val_recall_1: 0.9089 - val_precision_1: 0.9108\n",
      "Epoch 10/20\n",
      "100929/100929 [==============================] - 73s 719us/sample - loss: 0.0158 - acc: 0.9946 - recall_1: 0.9945 - precision_1: 0.9947 - val_loss: 0.4723 - val_acc: 0.9089 - val_recall_1: 0.9084 - val_precision_1: 0.9098\n",
      "Epoch 11/20\n",
      "100929/100929 [==============================] - 71s 708us/sample - loss: 0.0127 - acc: 0.9958 - recall_1: 0.9957 - precision_1: 0.9958 - val_loss: 0.5196 - val_acc: 0.9085 - val_recall_1: 0.9080 - val_precision_1: 0.9093\n",
      "Epoch 12/20\n",
      "100929/100929 [==============================] - 71s 700us/sample - loss: 0.0102 - acc: 0.9966 - recall_1: 0.9966 - precision_1: 0.9967 - val_loss: 0.5223 - val_acc: 0.9102 - val_recall_1: 0.9098 - val_precision_1: 0.9109\n",
      "Epoch 13/20\n",
      "100929/100929 [==============================] - 72s 710us/sample - loss: 0.0084 - acc: 0.9972 - recall_1: 0.9972 - precision_1: 0.9973 - val_loss: 0.5535 - val_acc: 0.9099 - val_recall_1: 0.9096 - val_precision_1: 0.9106\n",
      "Epoch 14/20\n",
      "100929/100929 [==============================] - 73s 721us/sample - loss: 0.0073 - acc: 0.9976 - recall_1: 0.9976 - precision_1: 0.9977 - val_loss: 0.5753 - val_acc: 0.9087 - val_recall_1: 0.9084 - val_precision_1: 0.9093\n",
      "Epoch 15/20\n",
      "100929/100929 [==============================] - 71s 707us/sample - loss: 0.0064 - acc: 0.9979 - recall_1: 0.9979 - precision_1: 0.9980 - val_loss: 0.5892 - val_acc: 0.9086 - val_recall_1: 0.9083 - val_precision_1: 0.9092\n",
      "Epoch 16/20\n",
      "100929/100929 [==============================] - 72s 709us/sample - loss: 0.0057 - acc: 0.9982 - recall_1: 0.9981 - precision_1: 0.9982 - val_loss: 0.5956 - val_acc: 0.9096 - val_recall_1: 0.9093 - val_precision_1: 0.9101\n",
      "Epoch 17/20\n",
      "100929/100929 [==============================] - 72s 709us/sample - loss: 0.0051 - acc: 0.9983 - recall_1: 0.9983 - precision_1: 0.9983 - val_loss: 0.6134 - val_acc: 0.9093 - val_recall_1: 0.9090 - val_precision_1: 0.9098\n",
      "Epoch 18/20\n",
      "100929/100929 [==============================] - 73s 725us/sample - loss: 0.0047 - acc: 0.9985 - recall_1: 0.9985 - precision_1: 0.9985 - val_loss: 0.6267 - val_acc: 0.9094 - val_recall_1: 0.9091 - val_precision_1: 0.9099\n",
      "Epoch 19/20\n",
      "100929/100929 [==============================] - 71s 706us/sample - loss: 0.0043 - acc: 0.9986 - recall_1: 0.9986 - precision_1: 0.9986 - val_loss: 0.6382 - val_acc: 0.9073 - val_recall_1: 0.9071 - val_precision_1: 0.9078\n",
      "Epoch 20/20\n",
      "100929/100929 [==============================] - 72s 710us/sample - loss: 0.0041 - acc: 0.9987 - recall_1: 0.9986 - precision_1: 0.9987 - val_loss: 0.6500 - val_acc: 0.9095 - val_recall_1: 0.9092 - val_precision_1: 0.9098\n",
      "*********** Training finished on:  2019-04-24 - 03:31:07\n",
      "Elapsed time:  1445.1305463314056  seconds ( 24.08550910552343  minutes).\n",
      "[INFO] Saving full model...\n",
      "[INFO] Model saved successfully.\n",
      "\n",
      "[INFO] Saving model plot...\n",
      "[INFO] Model plot saved.\n",
      "\n",
      "[INFO] Saving model weights...\n",
      "[INFO] Model weights saved successfully.\n",
      "\n",
      "[INFO] Saving history of the training...\n",
      "[INFO] History of the training successfully.\n",
      "\n",
      "Done.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "learn_rate=0.001\n",
    "dropout_LSTM = 0.6\n",
    "print(\"*********** Training started on: \", datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d - %H:%M:%S'))\n",
    "\n",
    "maxlength = 50\n",
    "tconfig = ModelConfig.train_config(tag=\"_GSA1.1.4\", dataset='all0.3', lenvocab=len(vocab), opt='adam', lr=learn_rate, drop_lstm=dropout_LSTM, drop_rec=0.0, emb_size1=64, emb_size2=64, batch_size=512, lstm_units=256,maxlength=50)\n",
    "print(\"Configuration: \")\n",
    "print(tconfig.__dict__)  \n",
    "model = ModelConfig.get_model_custom(tconfig)\n",
    "#model = toTPU(model)\n",
    "model_name = ModelConfig.get_model_name(tconfig)\n",
    "print(\"Model name: \" + model_name)\n",
    "#models[id].summary()\n",
    "tf_callback, time_callback, csv_logger = get_callbacks(model_name)\n",
    "start = time.time()\n",
    "hist = model.fit(xinput, yout, \n",
    "                         validation_data=(xtest,ytest),\n",
    "                         epochs=20,\n",
    "                         batch_size=tconfig.batch_size,\n",
    "                         callbacks=[tf_callback, time_callback, csv_logger],\n",
    "                         shuffle=True\n",
    "                        )\n",
    "\n",
    "end = time.time()-start\n",
    "print(\"*********** Training finished on: \", datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d - %H:%M:%S'))\n",
    "print(\"Elapsed time: \", str(end), \" seconds (\", str(end/60), \" minutes).\")\n",
    "H = hist\n",
    "hi = H.history\n",
    "hi['time_start'] = time_callback.epoch_time_start \n",
    "hi['elapsed'] = time_callback.times\n",
    "TrainingUtils.save_model_results(model,hist,root_path,model_name)\n",
    "print(\"Done.\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xTONUGElqqc9"
   },
   "source": [
    "## Training 1.2.1: LR:0.001, Drop_LSTM=0.2, Maxlength=30\n",
    "|   Static   | Variable                    |\n",
    "|:----------:|-----------------------------|\n",
    "| LR=0.001   | Drop_LSTM = [0,0.1,0.4,0.6] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RHw3OcsdqqdA"
   },
   "outputs": [],
   "source": [
    "xinput, yout, xtest, ytest, vocab = load_defaults_saved_all30()\n",
    "#x test is a list of unigram and bigrams like this [unigrams, bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "syYOFywpzsWR",
    "outputId": "fa5853be-e7eb-49f2-94f2-4a5f2cdfe11d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(xinput[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1183
    },
    "colab_type": "code",
    "id": "arzU98VIqqdF",
    "outputId": "65532630-eac9-4f4e-e35f-4b8ee37bf845"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** Training started on:  2019-04-24 - 10:19:46\n",
      "Configuration: \n",
      "{'tag': '_GSA1.2.1', 'dataset': 'all0.3', 'lenvocab': 1043604, 'opt': 'adam', 'lr': 0.0001, 'drop_lstm': 0.2, 'drop_rec': 0.0, 'emb_size1': 64, 'emb_size2': 64, 'batch_size': 512, 'lstm_units': 256, 'maxlength': 30}\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py:4010: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model name: 20190424__GSA1.2.1_all0.3_adam_lr0.0001_drLS0.2_drRec0.0_emb164_emb264_batch512_units256_maxl30\n",
      "Train on 169365 samples, validate on 21135 samples\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/20\n",
      "169365/169365 [==============================] - 83s 488us/sample - loss: 1.1460 - acc: 0.5193 - recall: 0.1171 - precision: 0.8709 - val_loss: 0.6907 - val_acc: 0.7593 - val_recall: 0.6252 - val_precision: 0.8639\n",
      "Epoch 2/20\n",
      "169365/169365 [==============================] - 81s 479us/sample - loss: 0.3767 - acc: 0.8676 - recall: 0.8250 - precision: 0.9048 - val_loss: 0.3271 - val_acc: 0.8854 - val_recall: 0.8699 - val_precision: 0.9009\n",
      "Epoch 3/20\n",
      "169365/169365 [==============================] - 81s 479us/sample - loss: 0.2252 - acc: 0.9195 - recall: 0.9058 - precision: 0.9323 - val_loss: 0.2888 - val_acc: 0.9014 - val_recall: 0.8923 - val_precision: 0.9111\n",
      "Epoch 4/20\n",
      "169365/169365 [==============================] - 82s 482us/sample - loss: 0.1762 - acc: 0.9367 - recall: 0.9290 - precision: 0.9439 - val_loss: 0.2881 - val_acc: 0.9057 - val_recall: 0.8996 - val_precision: 0.9121\n",
      "Epoch 5/20\n",
      "169365/169365 [==============================] - 81s 477us/sample - loss: 0.1471 - acc: 0.9468 - recall: 0.9420 - precision: 0.9514 - val_loss: 0.2821 - val_acc: 0.9093 - val_recall: 0.9046 - val_precision: 0.9144\n",
      "Epoch 6/20\n",
      "169365/169365 [==============================] - 81s 478us/sample - loss: 0.1265 - acc: 0.9539 - recall: 0.9506 - precision: 0.9571 - val_loss: 0.2772 - val_acc: 0.9109 - val_recall: 0.9071 - val_precision: 0.9151\n",
      "Epoch 7/20\n",
      "169365/169365 [==============================] - 81s 479us/sample - loss: 0.1103 - acc: 0.9595 - recall: 0.9572 - precision: 0.9619 - val_loss: 0.2814 - val_acc: 0.9112 - val_recall: 0.9083 - val_precision: 0.9148\n",
      "Epoch 8/20\n",
      "169365/169365 [==============================] - 81s 480us/sample - loss: 0.0963 - acc: 0.9647 - recall: 0.9629 - precision: 0.9664 - val_loss: 0.2893 - val_acc: 0.9117 - val_recall: 0.9093 - val_precision: 0.9146\n",
      "Epoch 9/20\n",
      "169365/169365 [==============================] - 81s 476us/sample - loss: 0.0840 - acc: 0.9693 - recall: 0.9680 - precision: 0.9707 - val_loss: 0.3035 - val_acc: 0.9110 - val_recall: 0.9091 - val_precision: 0.9136\n",
      "Epoch 10/20\n",
      "169365/169365 [==============================] - 80s 475us/sample - loss: 0.0725 - acc: 0.9738 - recall: 0.9727 - precision: 0.9748 - val_loss: 0.3206 - val_acc: 0.9103 - val_recall: 0.9086 - val_precision: 0.9124\n",
      "Epoch 11/20\n",
      "169365/169365 [==============================] - 81s 477us/sample - loss: 0.0616 - acc: 0.9780 - recall: 0.9773 - precision: 0.9788 - val_loss: 0.3328 - val_acc: 0.9100 - val_recall: 0.9085 - val_precision: 0.9119\n",
      "Epoch 12/20\n",
      "169365/169365 [==============================] - 81s 479us/sample - loss: 0.0516 - acc: 0.9820 - recall: 0.9814 - precision: 0.9826 - val_loss: 0.3501 - val_acc: 0.9095 - val_recall: 0.9083 - val_precision: 0.9113\n",
      "Epoch 13/20\n",
      "169365/169365 [==============================] - 81s 475us/sample - loss: 0.0428 - acc: 0.9854 - recall: 0.9849 - precision: 0.9858 - val_loss: 0.3829 - val_acc: 0.9079 - val_recall: 0.9069 - val_precision: 0.9095\n",
      "Epoch 14/20\n",
      "169365/169365 [==============================] - 80s 474us/sample - loss: 0.0356 - acc: 0.9881 - recall: 0.9878 - precision: 0.9885 - val_loss: 0.4009 - val_acc: 0.9074 - val_recall: 0.9066 - val_precision: 0.9088\n",
      "Epoch 15/20\n",
      "169365/169365 [==============================] - 81s 475us/sample - loss: 0.0296 - acc: 0.9903 - recall: 0.9901 - precision: 0.9906 - val_loss: 0.4348 - val_acc: 0.9058 - val_recall: 0.9051 - val_precision: 0.9070\n",
      "Epoch 16/20\n",
      "169365/169365 [==============================] - 81s 479us/sample - loss: 0.0244 - acc: 0.9923 - recall: 0.9921 - precision: 0.9925 - val_loss: 0.4401 - val_acc: 0.9068 - val_recall: 0.9061 - val_precision: 0.9079\n",
      "Epoch 17/20\n",
      "169365/169365 [==============================] - 80s 472us/sample - loss: 0.0204 - acc: 0.9937 - recall: 0.9935 - precision: 0.9939 - val_loss: 0.4620 - val_acc: 0.9062 - val_recall: 0.9056 - val_precision: 0.9073\n",
      "Epoch 18/20\n",
      "169365/169365 [==============================] - 80s 475us/sample - loss: 0.0172 - acc: 0.9947 - recall: 0.9946 - precision: 0.9949 - val_loss: 0.4866 - val_acc: 0.9058 - val_recall: 0.9052 - val_precision: 0.9067\n",
      "Epoch 19/20\n",
      "169365/169365 [==============================] - 80s 473us/sample - loss: 0.0145 - acc: 0.9957 - recall: 0.9956 - precision: 0.9958 - val_loss: 0.5047 - val_acc: 0.9056 - val_recall: 0.9050 - val_precision: 0.9064\n",
      "Epoch 20/20\n",
      "169365/169365 [==============================] - 81s 477us/sample - loss: 0.0124 - acc: 0.9963 - recall: 0.9963 - precision: 0.9964 - val_loss: 0.5215 - val_acc: 0.9056 - val_recall: 0.9051 - val_precision: 0.9064\n",
      "*********** Training finished on:  2019-04-24 - 10:46:51\n",
      "Elapsed time:  1624.0986766815186  seconds ( 27.06831127802531  minutes).\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-c605221a60b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mhi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mhi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'time_start'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_callback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch_time_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mhi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'elapsed'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime_callback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0mTrainingUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model_results\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhist\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mroot_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TimeHistory' object has no attribute 'epoch_time_start'"
     ]
    }
   ],
   "source": [
    "learn_rate=0.0001\n",
    "dropout_LSTM = 0.2\n",
    "print(\"*********** Training started on: \", datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d - %H:%M:%S'))\n",
    "\n",
    "maxlength = 30\n",
    "tconfig = train_config(tag=\"_GSA1.2.1\", dataset='all0.3', lenvocab=len(vocab), opt='adam', lr=learn_rate, drop_lstm=dropout_LSTM, drop_rec=0.0, emb_size1=64, emb_size2=64, batch_size=512, lstm_units=256,maxlength=maxlength)\n",
    "print(\"Configuration: \")\n",
    "print(tconfig.__dict__)  \n",
    "model = get_model_custom(tconfig)\n",
    "#model = toTPU(model)\n",
    "model_name = get_model_name(tconfig)\n",
    "print(\"Model name: \" + model_name)\n",
    "#models[id].summary()\n",
    "tf_callback, time_callback, csv_logger = get_callbacks(model_name)\n",
    "start = time.time()\n",
    "hist = model.fit(xinput, yout, \n",
    "                         validation_data=(xtest,ytest),\n",
    "                         epochs=20,\n",
    "                         batch_size=tconfig.batch_size,\n",
    "                        )\n",
    "\n",
    "end = time.time()-start\n",
    "print(\"*********** Training finished on: \", datetime.datetime.fromtimestamp(time.time()).strftime('%Y-%m-%d - %H:%M:%S'))\n",
    "print(\"Elapsed time: \", str(end), \" seconds (\", str(end/60), \" minutes).\")\n",
    "H = hist\n",
    "hi = H.history\n",
    "hi['time_start'] = time_callback.epoch_time_start \n",
    "hi['elapsed'] = time_callback.times\n",
    "TrainingUtils.save_model_results(model,hist,root_path,model_name)\n",
    "print(\"Done.\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BCUsRlXeqocW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "4HVu_d9STkL8",
    "bc4qYBRJjbNL",
    "ZZ3uH5Qbwu27",
    "Bi4NSKUeoVnn",
    "Bv39QAOezk4R",
    "zOiQu9U6_ejX",
    "-J_kSCYEGcu4",
    "vsRMWO-OSnpv",
    "xTONUGElqqc9"
   ],
   "name": "GridSearch1 (All)",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
